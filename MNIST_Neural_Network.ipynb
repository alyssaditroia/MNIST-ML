{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alyssaditroia/MNIST-ML/blob/main/MNIST_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfrhr2OWvgt"
      },
      "source": [
        "#**Core Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZxOUwIAUPCC"
      },
      "source": [
        "## Demo: MNIST Handwritten Digits\n",
        "\n",
        "The **MNIST dataset** is one of the most famous starting points in machine learning.  \n",
        "It contains **handwritten digits** (0 through 9), each stored as a **28×28 grayscale image**.  \n",
        "That means every digit is a tiny picture made of **784 pixels**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why MNIST?\n",
        "- It’s **visual**: you can actually *see* what the model is learning.  \n",
        "- It’s **simple but real**: the problem is meaningful (digit recognition) but small enough to run quickly on a laptop.  \n",
        "- It’s the **\"hello world\"** of computer vision: if your model works here, you’re ready to try harder datasets later.\n",
        "\n",
        "---\n",
        "\n",
        "### What the Data Looks Like\n",
        "- **Images:** Digits written in many different handwriting styles.  \n",
        "- **Labels:** The correct number (0–9).  \n",
        "- **Training set:** 60,000 examples.  \n",
        "- **Test set:** 10,000 new examples (to check generalization).  \n",
        "\n",
        "Usually we **normalize** the pixel values to the range 0–1 so training is smoother.\n",
        "\n",
        "---\n",
        "\n",
        "### What We’ll See in This Demo\n",
        "1. **Visualize the data**: show a small grid of raw digit images.  \n",
        "2. **Train simple models**:  \n",
        "   - Logistic regression (linear classifier).  \n",
        "   - A small multi-layer perceptron (MLP) with hidden layers.  \n",
        "   - Optionally, a simple convolutional neural net (CNN).  \n",
        "3. **Compare performance**:  \n",
        "   - Logistic regression: ~92% accuracy.  \n",
        "   - MLP: ~97–98%.  \n",
        "   - CNN: ~99%.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why This Is Useful\n",
        "- You’ll learn how **model capacity** affects performance: linear → hidden layers → convolution.  \n",
        "- You’ll see where models make mistakes (e.g., 3 vs 5, 4 vs 9).  \n",
        "- You’ll practice evaluating with metrics like **accuracy, confusion matrices, and per-class performance**.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "MNIST is small, visual, and fast — the perfect sandbox to understand how neural networks move from simple classifiers to powerful vision models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading MNIST Dataset"
      ],
      "metadata": {
        "id": "a-Dv3NP62drv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Normalize Pixel Values?\n",
        "\n",
        "Each MNIST image is made of **28×28 pixels**, where every pixel is a number between **0 and 255**:  \n",
        "- **0 = black** (no ink)  \n",
        "- **255 = white** (full ink intensity)  \n",
        "- Values in between = shades of gray  \n",
        "\n",
        "If we feed these raw values directly into a neural network, training becomes unstable:  \n",
        "- Large numbers (like 200+) can cause weight updates to explode.  \n",
        "- Different features (pixels) operate on different scales, which slows down learning.  \n",
        "\n",
        "**Normalization** fixes this:  \n",
        "- We divide every pixel by 255, rescaling all values to the range **0–1**.  \n",
        "- This makes training more stable, keeps weight updates consistent, and allows the optimizer to converge faster.  \n",
        "\n",
        "Think of it like grading: Instead of raw scores (e.g., 73/150), we convert everything to percentages (0–100%) so results are **comparable and fair**.  "
      ],
      "metadata": {
        "id": "ug3HuIia24mX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T7c28aNQmr-l",
        "outputId": "6275a71a-23b1-4c89-ecd9-b17bcb994763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and normalized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAADXdJREFUeJzt3FuIlWX7wOF7jVaakLgZQwpNUTMhS3LTxnJKS7EORqgQKvFkimxjoGUdpEYHNWElZRvBwsSOSkeCdgRpR+OuKNDcTJtpUMgtlmFZ4voOvv938++b6pt3dHZ6XeDJmvde77NQ1m89M85TKpfL5QCAiKjo6AUA0HmIAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAmekxsbGKJVKsWTJktP2nBs2bIhSqRQbNmw4bc8JnY0o0GmsXLkySqVSbN26taOX0i5uvvnmKJVK8eCDD3b0UiCJAnSAtWvXRn19fUcvA5oRBWhnv/32W8ybNy8WLFjQ0UuBZkSBLuX333+PhQsXxlVXXRW9e/eOXr16xfXXXx/r16//25kXX3wxBg8eHD179oxJkybFtm3bml2zc+fOuP3226Nv377Ro0ePGDt2bLz33nv/cz3Hjh2LnTt3xsGDB1v8Gp577rk4efJkzJ8/v8Uz0F5EgS7l559/jhUrVkRVVVXU1tbG4sWL48CBAzF16tT48ssvm12/atWqeOmll+KBBx6IJ554IrZt2xY33XRT7Nu3L6/Zvn17XH311bFjx454/PHH4/nnn49evXpFdXV11NXV/eN6Nm/eHJdddlksW7asRetvamqKZ599Nmpra6Nnz56FXju0h+4dvQAook+fPtHY2BjnnntuPlZTUxMjR46Ml19+Od54440/Xf/NN99EQ0NDXHTRRRERMW3atJgwYULU1tbGCy+8EBERc+fOjUGDBsWWLVvivPPOi4iIOXPmxMSJE2PBggUxY8aM07b+efPmxZgxY2LmzJmn7TnhdLJToEvp1q1bBuHkyZNx+PDhOHHiRIwdOza++OKLZtdXV1dnECIixo8fHxMmTIgPPvggIiIOHz4cn376adx5551x9OjROHjwYBw8eDAOHToUU6dOjYaGhti7d+/frqeqqirK5XIsXrz4f659/fr1sWbNmli6dGmxFw3tSBToct56660YPXp09OjRI/r16xeVlZXx/vvvx08//dTs2uHDhzd7bMSIEdHY2BgR/95JlMvlePLJJ6OysvJPfxYtWhQREfv37z/lNZ84cSIefvjhuOeee2LcuHGn/HzQVnz7iC5l9erVMXv27Kiuro5HH300BgwYEN26dYtnnnkmvv3228LPd/LkyYiImD9/fkydOvUvrxk2bNgprTni3z/b2LVrVyxfvjyD9B9Hjx6NxsbGGDBgQJx//vmnfC84FaJAl/Luu+/G0KFDY+3atVEqlfLx/3yq/28NDQ3NHtu9e3dccsklERExdOjQiIg455xzYsqUKad/wf+nqakp/vjjj7juuuuafW3VqlWxatWqqKuri+rq6jZbA7SEKNCldOvWLSIiyuVyRmHTpk1RX18fgwYNanb9unXrYu/evflzhc2bN8emTZvikUceiYiIAQMGRFVVVSxfvjweeuihGDhw4J/mDxw4EJWVlX+7nmPHjkVTU1P0798/+vfv/7fXzZw5M6688spmj8+YMSOmT58eNTU1MWHChH987dAeRIFO580334yPPvqo2eNz586N2267LdauXRszZsyIW2+9Nb7//vt4/fXXY9SoUfHLL780mxk2bFhMnDgx7r///jh+/HgsXbo0+vXrF4899lhe88orr8TEiRPj8ssvj5qamhg6dGjs27cv6uvrY8+ePfHVV1/97Vo3b94cN954YyxatOgff9g8cuTIGDly5F9+bciQIXYIdBqiQKfz2muv/eXjs2fPjtmzZ8ePP/4Yy5cvj48//jhGjRoVq1evjnfeeecvD6qbNWtWVFRUxNKlS2P//v0xfvz4WLZs2Z92BKNGjYqtW7fGU089FStXroxDhw7FgAEDYsyYMbFw4cK2epnQKZXK5XK5oxcBQOfgv6QCkEQBgCQKACRRACCJAgBJFABILf49hf9/pAAAXU9LfgPBTgGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA1L2jFwB0fZMnTy488/bbb7fqXpMmTSo8s2vXrlbd62xkpwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgHRWH4h3ww03FJ7p169f4Zm6urrCM9CVjBs3rvDMli1b2mAlnCo7BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApLP6QLyqqqrCM8OHDy8840A8upKKiuKfFYcMGVJ4ZvDgwYVnIiJKpVKr5mgZOwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCd1aekzpo1q/BMfX19G6wEOo+BAwcWnqmpqSk8s3r16sIzERE7d+5s1RwtY6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYB0Vh+IV1GhifDfVqxY0S73aWhoaJf7UIx3RQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApDPmQLzRo0cXnrnwwgvbYCXQtfXu3btd7vPJJ5+0y30oxk4BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDpjDkQb/r06YVnevbs2QYrgc6jNYc+DhkypA1W0tzevXvb5T4UY6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkM+aU1EsvvbRd7rN9+/Z2uQ+cDkuWLCk805qTVXfv3l145ujRo4VnaHt2CgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASGfMgXjtZcuWLR29BDqRCy64oPDMtGnTWnWvu+++u/DMLbfc0qp7FfX0008Xnjly5MjpXwinzE4BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJgXgF9e3bt6OXcNpdccUVhWdKpVLhmSlTphSeiYi4+OKLC8+ce+65hWfuuuuuwjMVFcU/V/3666+FZyIiNm3aVHjm+PHjhWe6dy/+tvD5558XnqFzslMAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEAqlcvlcosubMUBaO3p1VdfLTxz3333FZ45cuRI4ZmmpqbCM+1p9OjRhWda8+/hxIkThWciIo4dO1Z45uuvvy4805oD57Zu3Vp45rPPPis8ExGxb9++wjN79uwpPNOnT5/CM605gJD215K3ezsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCk7h29gNNlzpw5hWd++OGHwjPXXntt4ZnOrjUH9q1bt67wzI4dOwrPRERs3LixVXNnmnvvvbfwTGVlZeGZ7777rvAMZw47BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIJ0xp6S2Rm1tbUcvAVps8uTJ7XKfNWvWtMt96JzsFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkM7qA/GA5urq6jp6CXQgOwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUveOXgDQdkqlUuGZESNGFJ7ZuHFj4Rk6JzsFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkB+LBGaxcLheeqajwWfFs5m8fgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABITkkF/uSaa64pPLNy5crTvxA6hJ0CAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSA/HgDFYqlTp6CXQxdgoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEgOxIMu4sMPPyw8c8cdd7TBSjiT2SkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCVyuVyuUUXlkptvRYA2lBL3u7tFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACB1b+mF5XK5LdcBQCdgpwBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA+hdjFzd/d+BizQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAADyFJREFUeJzt3H2s1/P/x/HnR/lWNFKdtmYrO8qI2hCZpXIxuYidRLGZFeuPmJktlyMMw+b6uomVZTsLJWLSRq7WSq62Q5GLZq5Pkmsafb5/fH+e41c4r0/nopPbbfPP8X70fjmqe+9TvSvVarUaABARO3T0AQDYdogCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkC26W1a9dGpVKJG2+8sdW+zaVLl0alUomlS5e22rcJ2xpRYJsxe/bsqFQqsXLlyo4+SptpbGyMAw44ILp37x51dXVx1llnxbp16zr6WJBEAdrJPffcE6eddlr07t07br755pg6dWo0NjbGkUceGT///HNHHw8iIqJrRx8A/g02btwYl156aYwaNSqWLFkSlUolIiIOPfTQOOGEE+K+++6Lc889t4NPCZ4U6GQ2btwYM2bMiAMPPDB23XXX2HnnneOwww6L55577i83t9xySwwcODB69OgRo0ePjqamps2uWb16dZx88snRu3fv6N69ewwfPjwef/zxfzzPjz/+GKtXr/7HLwE1NTXFhg0bYtKkSRmEiIhx48ZFz549o7Gx8R/vBe1BFOhUvv3225g1a1aMGTMmbrjhhrjyyiujubk5xo4dG2+88cZm1z/44INx++23xznnnBOXXHJJNDU1xRFHHBFffPFFXvPWW2/FIYccEqtWrYqLL744brrppth5552joaEhFixY8LfnWbFiReyzzz5x5513/u11v/zyS0RE9OjRY7N/16NHj3j99ddj06ZNLfgMQNvy5SM6ld122y3Wrl0b//nPf/JjU6dOjb333jvuuOOOuP/++/90/XvvvRdr1qyJ3XffPSIijjnmmBgxYkTccMMNcfPNN0dExHnnnRcDBgyIV155Jbp16xYREWeffXaMHDkyLrroohg/fvxWn3vw4MFRqVTi5ZdfjilTpuTH33nnnWhubo6IiK+//jr69Omz1feCreFJgU6lS5cuGYRNmzbF+vXr49dff43hw4fHa6+9ttn1DQ0NGYSIiIMPPjhGjBgRTz31VERErF+/Pp599tmYOHFifPfdd7Fu3bpYt25dfPXVVzF27NhYs2ZNfPLJJ395njFjxkS1Wo0rr7zyb8/dt2/fmDhxYsyZMyduuumm+OCDD+LFF1+MSZMmxY477hgRET/99FPppwNanSjQ6cyZMyeGDRsW3bt3jz59+kRdXV08+eST8c0332x27eDBgzf72F577RVr166NiP89SVSr1bj88sujrq7uT/9cccUVERHx5Zdftsq5Z86cGccdd1xMnz499txzzxg1alQMHTo0TjjhhIiI6NmzZ6vcB7aGLx/RqcydOzcmT54cDQ0NccEFF0S/fv2iS5cucd1118X7779f/O39/nX86dOnx9ixY7d4zaBBg7bqzL/bddddY+HChfHRRx/F2rVrY+DAgTFw4MA49NBDo66uLnr16tUq94GtIQp0Ko888kjU19fH/Pnz//SneH7/Vf3/t2bNms0+9u6778Yee+wRERH19fUREbHjjjvGUUcd1foH3oIBAwbEgAEDIiJiw4YN8eqrr8aECRPa5d7wT3z5iE6lS5cuERFRrVbzY8uXL49ly5Zt8frHHnvsT78nsGLFili+fHkce+yxERHRr1+/GDNmTMycOTM+++yzzfa//ybwX2npH0n9K5dcckn8+uuvcf7559e0h9bmSYFtzgMPPBBPP/30Zh8/77zzYty4cTF//vwYP358HH/88fHhhx/GvffeG0OGDInvv/9+s82gQYNi5MiRMW3atPjll1/i1ltvjT59+sSFF16Y19x1110xcuTIGDp0aEydOjXq6+vjiy++iGXLlsXHH38cb7755l+edcWKFXH44YfHFVdc8Y+/2Xz99ddHU1NTjBgxIrp27RqPPfZYPPPMM3HNNdfEQQcd1PJPELQhUWCbc88992zx45MnT47JkyfH559/HjNnzozFixfHkCFDYu7cufHwww9v8UV1Z5xxRuywww5x6623xpdffhkHH3xw3HnnndG/f/+8ZsiQIbFy5cq46qqrYvbs2fHVV19Fv379Yv/9948ZM2a02n/X0KFDY8GCBfH444/Hb7/9FsOGDYt58+bFKaec0mr3gK1Vqf7xORyAfzW/pwBAEgUAkigAkEQBgCQKACRRACC1+O8p/PGVAgB0Pi35GwieFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUteOPgD8kxEjRhRvTj/99OLN6NGjizf77rtv8aZW06dPL958+umnxZuRI0cWb+bOnVu8Wb58efGGtudJAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASJVqtVpt0YWVSlufhe3cpEmTatrddtttxZu+ffsWb2r5Pr506dLiTV1dXfEmImLIkCE17UrV8nl4+OGHizennnpq8Yat05Kf7j0pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgde3oA9DxunYt/24wfPjw4s19991XvImI2GmnnYo3L7zwQvHm6quvLt689NJLxZtu3boVbyIi5s2bV7w5+uija7pXqZUrV7bLfWh7nhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJC8EI84/fTTizezZs1qg5Ns2ZIlS4o3kyZNKt58++23xZta1HK2iPZ7ud3HH39cvJkzZ04bnISO4EkBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpUq1Wqy26sFJp67PQCq6++urizaWXXlq8aeF3mz+5++67izcREZdddlnxpr1ebleLVatW1bQbPHhwK59kyyZMmFC8WbhwYRuchNbWkh+3nhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUtaMPwJbNmDGjpl0tbzzduHFj8Wbx4sXFm4suuqh4ExHx008/1bQr1b179+LN0UcfXbwZMGBA8SaitjcVX3PNNcUbbzz9d/OkAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAVKlWq9UWXVjDy7j4n169ehVvVq9eXdO9+vbtW7xZtGhR8aahoaF4054GDRpUvHnooYeKNwceeGDxplaPPvpo8ebMM88s3vzwww/FGzqHlvx070kBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJC/HaQb9+/Yo3n376aRucZMvq6+uLNz///HPxZsqUKcWbiIgTTzyxeLPffvsVb3r27Fm8aeEPn63eREScdNJJxZsnnniipnuxffJCPACKiAIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQPJCvHbQq1ev4s2qVatqulddXV3xppb/t7W+1K291PJCwVo+D/379y/eNDc3F29qvRf8kRfiAVBEFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUteOPsC/wYYNG4o3DQ0NNd1r0aJFxZvevXsXb95///3izcKFC4s3ERGzZ88u3qxfv75409jYWLyp5SV1tdwH2osnBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIHlL6jZq+fLlNe3q6upa+SSd06hRo4o3o0ePLt5s2rSpePPBBx8Ub6C9eFIAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDyQjy2Sz169Cje1PJyu2q1WrxpbGws3kB78aQAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUqbbwjV6VSqWtzwId6rfffive1PJCvP79+xdvIiKam5tr2sHvWvL91ZMCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBS144+ALSFsWPHdvQRoFPypABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOSFeGyX6uvrO/oI0Cl5UgAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJK3pLJdevHFF4s3O+xQ/mukTZs2FW9gW+ZJAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyQvx2C41NTUVb9asWVO8qa+vL97sueeexZuIiObm5pp2UMKTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUqVarVZbdGGl0tZngQ41efLk4s2sWbOKN88//3zxJiLi3HPPLd68/fbbNd2L7VNLfrr3pABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOSFePB/dtlll+LNvHnzijdHHXVU8SYiYv78+cWbKVOmFG9++OGH4g2dgxfiAVBEFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkLwlFbZCLW9Wvfbaa2u617Rp04o3w4YNK968/fbbxRs6B29JBaCIKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJC/EA/iX8EI8AIqIAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA6trSC1v43jwAOjFPCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCk/wI4JPMQokgraAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# --- Import necessary libraries ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist         # Built-in MNIST dataset\n",
        "from tensorflow.keras.models import Sequential      # For building a simple neural network\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam        # Common optimizer\n",
        "from tensorflow.keras.initializers import RandomNormal, GlorotNormal, Constant\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Load and preprocess the MNIST dataset ---\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Each pixel is originally between 0 and 255 (grayscale intensity).\n",
        "# Neural networks train better when inputs are small and consistent,\n",
        "# so we normalize to the range 0–1 by dividing by 255.\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# --- Create a validation split ---\n",
        "# We take the last 10,000 examples from the training set\n",
        "# and set them aside for validation (to tune hyperparameters).\n",
        "x_val, y_val = x_train[-10000:], y_train[-10000:]\n",
        "x_train, y_train = x_train[:-10000], y_train[:-10000]\n",
        "\n",
        "print(\"Data loaded and normalized.\")\n",
        "\n",
        "# --- Visualize the data ---\n",
        "# Show the first five training examples so we can see what the images look like.\n",
        "for i in range(5):\n",
        "    plt.imshow(x_train[i], cmap='gray')   # Display the image in grayscale\n",
        "    plt.title(f\"Label: {y_train[i]}\")     # The correct digit (label)\n",
        "    plt.axis('off')                       # Hide the axes for clarity\n",
        "    plt.show()\n",
        "\n",
        "# Notes:\n",
        "# - x_train[i] is a 28x28 array of pixel values (0–1 after normalization).\n",
        "# - y_train[i] is the corresponding label (0–9).\n",
        "# - This visualization step is important: always LOOK at your data before training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v32gzHaF3P-u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vfmKUGLUS6z"
      },
      "source": [
        "#**Plot Learing Curves**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Learning Curves\n",
        "\n",
        "Training a neural network is like watching a student practice:  \n",
        "we want to see **if they’re improving** and **when they start to memorize instead of generalize**.  \n",
        "\n",
        "This helper function plots two key curves over time (epochs):  \n",
        "\n",
        "### 1. Accuracy (left plot)\n",
        "- **Train Accuracy**: how often the model gets the training examples right.  \n",
        "- **Validation Accuracy**: how often the model gets *new, unseen* validation examples right.  \n",
        "- When both go up and level off together → the model is learning well.  \n",
        "- If training keeps improving but validation stalls or drops → the model is overfitting.  \n",
        "\n",
        "### 2. Loss (right plot)\n",
        "- **Loss** measures how far predictions are from the true answers.  \n",
        "- **Train Loss**: goes down as the model fits the training data.  \n",
        "- **Validation Loss**: should follow a similar downward trend if the model generalizes.  \n",
        "- If validation loss rises while training loss falls → overfitting warning.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "These plots let us **diagnose learning**:  \n",
        "- Is the model still improving?  \n",
        "- Has it plateaued?  \n",
        "- Is it memorizing training data instead of generalizing?  \n",
        "\n",
        "Always check these curves to decide if you need more epochs, regularization, or a different architecture.  "
      ],
      "metadata": {
        "id": "3wDrn21q3Tdk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiYRCTlim0WT",
        "outputId": "c3798bdf-27e5-4879-dcf9-ac7de0417180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper function defined.\n"
          ]
        }
      ],
      "source": [
        "# Helper function to plot training and validation learning curves\n",
        "def plot_learning_curves(history):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "print(\"Helper function defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0LD9dIZUcdv"
      },
      "source": [
        "#**Model Building Function with Adjustable Hyperparameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Neural Network Model (Configurable)\n",
        "\n",
        "This function creates a **fully connected neural network** for the MNIST digit classification task.  \n",
        "It is written to be flexible, so we can easily experiment with different hyperparameters.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Flatten the Input\n",
        "- Each MNIST digit is a **28×28 image**.  \n",
        "- We flatten it into a **784-length vector** (28 × 28) so it can feed into dense (fully connected) layers.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Hidden Layers\n",
        "- We can add **one or more hidden layers** depending on the parameter `num_hidden_layers`.  \n",
        "- Each hidden layer:  \n",
        "  - Has a chosen number of **neurons** (`num_neurons`).  \n",
        "  - Uses an **activation function** (e.g., `relu`) to introduce non-linearity.  \n",
        "  - Initializes **weights** with `glorot_normal` (good default) or `random_normal`.  \n",
        "  - Initializes **biases** with a constant (default = 0).  \n",
        "- Optionally, we can apply **Dropout** after each hidden layer to reduce overfitting.  \n",
        "  - `dropout_rate = 0.0` means no dropout.  \n",
        "  - Higher values (e.g., 0.2 or 0.5) randomly \"turn off\" some neurons during training, which forces the network to generalize.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Output Layer\n",
        "- Final layer has **10 neurons** (one for each digit 0–9).  \n",
        "- Uses a **softmax activation** so the outputs form a probability distribution (all add up to 1).  \n",
        "- Example: [0.01, 0.02, 0.85, 0.01, ...] → model predicts \"digit 2\" with 85% confidence.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why This Matters\n",
        "- This design lets us **experiment with architecture choices** (neurons, layers, activation functions).  \n",
        "- By adjusting hyperparameters, students can see how model complexity, dropout, and initialization affect performance.  \n",
        "- The same structure scales to more complex problems (e.g., bigger images, different datasets).  \n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "This function is a **blueprint for building neural networks** — we can swap parts in and out to test how design decisions change learning.  "
      ],
      "metadata": {
        "id": "2DeMTX_w3ejq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQF6K1wYm3p_",
        "outputId": "1e6ec64b-58d7-4561-e109-c3e0be5ca3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model building function defined.\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Dropout\n",
        "from keras.initializers import RandomNormal, GlorotNormal, Constant\n",
        "\n",
        "# Function to build the model with adjustable hyperparameters\n",
        "def build_model(\n",
        "    num_neurons=128,  # Number of neurons in the hidden layer\n",
        "    num_hidden_layers=1,  # Number of hidden layers\n",
        "    init_weights='glorot_normal',  # Weight initialization\n",
        "    init_bias=0.0,  # Initial bias\n",
        "    dropout_rate=0.0,  # Dropout rate\n",
        "    activation='relu'  # Activation function\n",
        "):\n",
        "    # Set weight initializer\n",
        "    initializer = RandomNormal(mean=0., stddev=0.05) if init_weights == 'random_normal' else GlorotNormal()\n",
        "\n",
        "    # Set bias initializer\n",
        "    bias_initializer = Constant(init_bias)\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    # Add hidden layers based on configuration\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(Dense(num_neurons, activation=activation, kernel_initializer=initializer, bias_initializer=bias_initializer))\n",
        "        if dropout_rate > 0:\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer with 10 neurons (for 10 digits)\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"Model building function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1T5-nD2U01g"
      },
      "source": [
        "#**Model Training Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling and Training the Model\n",
        "\n",
        "Once we build a model, we need to **tell it how to learn**.  \n",
        "This function sets the learning rules (compiling) and then trains the model on the MNIST dataset.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Compile the Model\n",
        "- **Optimizer:** `Adam(learning_rate=...)`  \n",
        "  - The optimizer decides how weights are updated during training.  \n",
        "  - Adam is a popular choice because it adapts the learning rate for each weight automatically.  \n",
        "- **Loss function:** `sparse_categorical_crossentropy`  \n",
        "  - Measures how far the predicted probability distribution is from the true label.  \n",
        "  - Best for multi-class problems like MNIST (10 digits).  \n",
        "- **Metrics:** `accuracy`  \n",
        "  - Tracks what fraction of predictions are correct.  \n",
        "  - This is easy to interpret while training.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Train the Model\n",
        "- **Epochs:** number of full passes through the training data.  \n",
        "- **Batch size:** how many images are processed at once before updating weights.  \n",
        "- **Validation data:** a separate set the model has not seen during training.  \n",
        "  - This helps us check if the model generalizes or if it’s overfitting.  \n",
        "\n",
        "The training loop:  \n",
        "1. Take a batch of training data.  \n",
        "2. Predict digit probabilities.  \n",
        "3. Compare predictions to true labels using the loss function.  \n",
        "4. Use the optimizer to adjust weights.  \n",
        "5. Repeat across all batches for the chosen number of epochs.  \n",
        "\n",
        "---\n",
        "\n",
        "### What We Get Back\n",
        "- The function returns a **history object**, which records:  \n",
        "  - Accuracy and loss on both training and validation sets across epochs.  \n",
        "  - This is what we plot in the **learning curves** to analyze progress.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "Compiling sets the *rules of learning* (optimizer, loss, metrics).  \n",
        "Training applies those rules, adjusting the network step by step until it recognizes digits with high accuracy.  "
      ],
      "metadata": {
        "id": "jFtnaARz3lWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wkdjaVLm7v6",
        "outputId": "c932205d-8caf-4674-e05d-dec76622cf46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training function defined.\n"
          ]
        }
      ],
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "# Function to compile and train the model\n",
        "def train_model(\n",
        "    model,  # The built model to be trained\n",
        "    learning_rate=0.001,  # Learning rate\n",
        "    batch_size=64,  # Batch size\n",
        "    epochs=5  # Number of epochs\n",
        "):\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"Model training function defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4b4KC2mkVXu"
      },
      "source": [
        "#**Baseline Comparison to other Classification Techniques**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln5tQn1pdj8u"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VqtAt32gJis"
      },
      "outputs": [],
      "source": [
        "# Flatten the images for use in scikit-learn models\n",
        "x_train_flat = x_train.reshape(-1, 28 * 28)  # Shape: (num_samples, 784)\n",
        "x_test_flat = x_test.reshape(-1, 28 * 28)    # Shape: (num_samples, 784)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Simple Classifiers on MNIST\n",
        "\n",
        "Before using deep neural networks, it’s helpful to try out some **traditional machine learning models**.  \n",
        "This code trains two basic classifiers on MNIST and compares their performance.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Define Classifiers\n",
        "- **Logistic Regression**  \n",
        "  - A linear model that works well for simple classification tasks.  \n",
        "  - On MNIST, it tries to draw linear boundaries in pixel space to separate digits.  \n",
        "- **Decision Tree**  \n",
        "  - Splits the data into rules (like “if pixel [15,10] is dark → maybe it’s a 7”).  \n",
        "  - Trees are flexible, but can overfit if not controlled.  \n",
        "\n",
        "Both are simple baselines: they help us see what accuracy looks like **without deep learning**.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Training Each Model\n",
        "- For each classifier:  \n",
        "  - `fit`: train on the training set.  \n",
        "  - `predict`: use the trained model to make predictions on the test set.  \n",
        "  - `accuracy_score`: measure how many predictions were correct.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Compare Results\n",
        "- The code stores each model’s test accuracy in a dictionary.  \n",
        "- At the end, it prints something like:  "
      ],
      "metadata": {
        "id": "dDoNOJ_Z39FN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLQHn8CvgPrA"
      },
      "outputs": [],
      "source": [
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "\n",
        "}\n",
        "\n",
        "# Dictionary to store accuracy results\n",
        "accuracy_results = {}\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "for name, clf in classifiers.items():\n",
        "    clf.fit(x_train_flat, y_train)  # Train the model\n",
        "    predictions = clf.predict(x_test_flat)  # Make predictions\n",
        "    accuracy = accuracy_score(y_test, predictions)  # Calculate accuracy\n",
        "    accuracy_results[name] = accuracy  # Store accuracy results\n",
        "\n",
        "# Display accuracy results\n",
        "for name, accuracy in accuracy_results.items():\n",
        "    print(f\"{name}: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting Baseline Results\n",
        "\n",
        "- **Logistic Regression Accuracy: 92.5%**  \n",
        "  - This simple linear model already does quite well on MNIST.  \n",
        "  - It learns a set of weights for each pixel and draws straight-line boundaries in the high-dimensional pixel space.  \n",
        "  - Strength: captures overall structure in the images.  \n",
        "  - Limitation: struggles with digits that need more flexible, curved boundaries (like distinguishing a sloppy “4” from a “9”).  \n",
        "\n",
        "- **Decision Tree Accuracy: 87.1%**  \n",
        "  - The tree splits the image into pixel-based rules (e.g., “if this region is dark → likely a 7”).  \n",
        "  - Strength: very interpretable; you can trace decisions step by step.  \n",
        "  - Limitation: tends to **overfit** — it memorizes pixel quirks instead of generalizing well.  \n",
        "\n",
        "---\n",
        "\n",
        "### What This Comparison Shows\n",
        "- Logistic regression is stronger for MNIST because the data has enough linear structure for it to capture.  \n",
        "- Decision trees can still recognize many digits but are less reliable when the handwriting varies.  \n",
        "- Both give us useful **baselines**: they set a performance bar (around 90%).  \n",
        "\n",
        "**Key takeaway:**  \n",
        "Deep neural networks are needed to push accuracy higher (97–99%).  \n",
        "These baselines remind us that the real value of neural nets is in capturing the **curved, complex patterns** that simple models miss.  "
      ],
      "metadata": {
        "id": "HlqRCoLE4T80"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkfylsgyvXjZ"
      },
      "source": [
        "#**Prediction Probabilities for Random Test Image**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Prediction Probabilities\n",
        "\n",
        "When a neural network classifies a digit, it doesn’t just give a single answer.  \n",
        "It outputs a **probability distribution** over all 10 digits (0–9).  \n",
        "This function shows those probabilities for one image.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Prepare the Image\n",
        "- `image.reshape(1, 28, 28)`  \n",
        "  - The model expects input in batches (even if it’s just one image).  \n",
        "  - We reshape the 28×28 image into a batch of size 1.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Get Predictions\n",
        "- `model.predict(...)`  \n",
        "  - Returns 10 numbers (one for each digit).  \n",
        "  - Each number is between 0 and 1, and all 10 add up to 1.  \n",
        "  - Example: [0.01, 0.02, 0.85, 0.05, ...] means the model thinks \"digit 2\" is most likely.  \n",
        "- `np.argmax(probabilities)`  \n",
        "  - Finds the digit with the **highest probability** = the model’s predicted label.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Plot the Probabilities\n",
        "- Creates a bar chart showing how confident the model is for each digit.  \n",
        "- **X-axis:** digits 0–9.  \n",
        "- **Y-axis:** probability (0–1).  \n",
        "- The true label and predicted label are shown in the plot title.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why This Is Useful\n",
        "- Instead of just seeing the final answer, we see the **model’s confidence distribution**.  \n",
        "- If the model predicts “3” but also gives “5” a high probability, it shows the model sees the similarity between them.  \n",
        "- Helps us spot when the model is **confident but wrong** (overconfident mistakes).  \n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "Classification isn’t just about the predicted digit — it’s about the **probabilities behind the prediction**, which reveal how the model is reasoning.  "
      ],
      "metadata": {
        "id": "SRh0zq3c4DOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFX0lhoGjKo5"
      },
      "outputs": [],
      "source": [
        "def display_prediction_probabilities(model, image, true_label):\n",
        "    image_reshaped = image.reshape(1, 28, 28)  # Reshape to (1, 28, 28)\n",
        "    probabilities = model.predict(image_reshaped)\n",
        "    probabilities = probabilities[0]  # Get the probabilities for the single image\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(10), probabilities, color='blue', alpha=0.7)\n",
        "    plt.xticks(range(10), [str(i) for i in range(10)])\n",
        "    plt.xlabel('Digits')\n",
        "    plt.ylabel('Probability')\n",
        "    predicted_label = np.argmax(probabilities)\n",
        "    plt.title(f'True Label: {true_label}, Predicted Label: {predicted_label}')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What This Helper Function Does\n",
        "\n",
        "The `display_prediction_probabilities` function is designed to **visualize how the model \"thinks\" about a single image**.  \n",
        "\n",
        "Instead of only giving the final answer (e.g., \"This is a 7\"), it shows the **full probability distribution** across all 10 digits (0–9).  \n",
        "\n",
        "- **Input:**  \n",
        "  - A trained model.  \n",
        "  - One handwritten digit image (28×28 pixels).  \n",
        "  - The true label (the correct digit).  \n",
        "\n",
        "- **Process:**  \n",
        "  - Reshapes the image so the model can process it.  \n",
        "  - Gets the model’s probability predictions for each digit.  \n",
        "  - Finds the predicted label (the digit with the highest probability).  \n",
        "  - Draws a **bar chart** of all 10 probabilities.  \n",
        "\n",
        "- **Output:**  \n",
        "  - A visualization showing:  \n",
        "    - The model’s predicted digit.  \n",
        "    - The true digit.  \n",
        "    - How confident the model was in its decision.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why It’s Useful\n",
        "- Lets us see **confidence, not just correctness**.  \n",
        "- Helps identify when the model is **unsure** (two or more bars are close in height).  \n",
        "- Highlights **overconfident mistakes** (when the wrong bar is very high).  \n",
        "\n",
        "**In short:**  \n",
        "This helper function is designed to open up the \"black box\" of the neural network and show its reasoning in a visual, student-friendly way.  "
      ],
      "metadata": {
        "id": "nyzHP3R35W0x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz95o1OzWr0C"
      },
      "source": [
        "#**Neural Network Experiments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyCWP2OudBtq"
      },
      "source": [
        "* **Experiment 0**: Running the Model with 0 Neurons\n",
        "* **Experiment 1**: Running the Model with 64 Neurons\n",
        "* **Experiment 2**: Running the Model with 128 Neurons (Baseline)\n",
        "* **Experiment 3**: Running the Model with 256 Neurons\n",
        "* **Experiment 4**: Running the Model with 1 Hidden Layer\n",
        "* **Experiment 5**: Running the Model with 2 Hidden Layers\n",
        "* **Experiment 6**: Running the Model with 3 Hidden Layers\n",
        "* **Experiment 7**: Running the Model with Dropout (0.2)\n",
        "* **Experiment 8**: Running the Model with Dropout (0.5)\n",
        "* **Experiment 9**: Running with a Smaller Learning Rate (0.0001)\n",
        "* **Experiment 10**: Running with a Larger Learning Rate (0.01)\n",
        "* **Experiment 11**: Running with Smaller Batch Size (32)\n",
        "* **Experiment 12**: Running with Larger Batch Size (128)\n",
        "* **Experiment 13**: Using Sigmoid Activation Instead of ReLU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting the Experiments\n",
        "\n",
        "In these experiments, we changed **one hyperparameter at a time** (neurons, layers, dropout, learning rate, batch size, activation).  \n",
        "The point is not just to see numbers go up or down, but to understand *how each change affects the way the model learns*.  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Number of Neurons\n",
        "Neurons determine how much capacity the network has to learn patterns.  \n",
        "\n",
        "- **0 neurons:** The model cannot learn — accuracy is random.  \n",
        "- **64 → 128 → 256 neurons:** Adding neurons increases the network’s representational power. Accuracy improves because the model can capture more variation in the data.  \n",
        "- **Too many neurons:** Training takes longer, and the model may start to overfit. Beyond a certain point, more neurons do not bring meaningful improvements.  \n",
        "\n",
        "**Explanation:** More neurons = more learning capacity, but diminishing returns appear once the dataset’s complexity is already captured.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Hidden Layers\n",
        "Hidden layers control the depth of the network.  \n",
        "\n",
        "- **1 hidden layer:** Often enough for MNIST, since the dataset is relatively simple. A single hidden layer already learns most digit patterns.  \n",
        "- **2–3 hidden layers:** May provide slight improvements but not dramatically so. The added depth is unnecessary for MNIST.  \n",
        "- **Too many layers:** Extra depth increases training time and risks overfitting without clear gains.  \n",
        "\n",
        "**Explanation:** Deeper networks can capture more complex patterns, but for MNIST one hidden layer is usually sufficient.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Dropout (Regularization)\n",
        "Dropout randomly “turns off” a fraction of neurons during training.  \n",
        "\n",
        "- **Dropout = 0.2:** A small amount of regularization helps prevent overfitting and improves generalization.  \n",
        "- **Dropout = 0.5:** Too many neurons are removed at once, making it harder for the network to learn strong features. Accuracy may drop.  \n",
        "\n",
        "**Explanation:** Dropout acts like controlled forgetting — a little bit improves robustness, too much weakens learning.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Learning Rate (Step Size)\n",
        "The learning rate controls how big each step is when updating weights.  \n",
        "\n",
        "- **Too small (0.0001):** Training is extremely slow. Accuracy barely improves within a few epochs.  \n",
        "- **Too large (0.01):** The model overshoots the optimal weight values, bouncing around and possibly failing to converge.  \n",
        "- **Baseline (0.001):** Balanced — stable and efficient convergence.  \n",
        "\n",
        "**Explanation:** The learning rate is like the stride you take when walking downhill. Too short = crawling, too long = stumbling, just right = smooth descent.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Batch Size (Update Frequency)\n",
        "Batch size is how many samples the model processes before making one update.  \n",
        "\n",
        "- **Small batch (32):** Produces more frequent, noisier updates. Training is slower, but the noise can help the model generalize better.  \n",
        "- **Large batch (128):** Produces fewer, smoother updates. Training is faster per epoch, but the model may generalize slightly worse.  \n",
        "- **Baseline (64):** A balanced setting that usually works best in practice.  \n",
        "\n",
        "**Explanation:** Smaller batches inject useful randomness, while larger batches train faster but sometimes reduce flexibility.  \n",
        "\n",
        "---\n",
        "\n",
        "### 6. Activation Function\n",
        "Activations add non-linearity so the network can capture complex relationships.  \n",
        "\n",
        "- **ReLU (Rectified Linear Unit):** Zero for negative values, linear for positive values. Fast and effective, avoids gradient saturation, and is the best choice for hidden layers.  \n",
        "- **Sigmoid:** Squashes values into (0,1). Useful for outputs when modeling probabilities, but in hidden layers it saturates and gradients vanish, slowing learning.  \n",
        "\n",
        "**Explanation:** ReLU keeps gradients flowing and speeds up training, while sigmoid tends to bottleneck deeper layers.  \n",
        "\n",
        "---\n",
        "\n",
        "## Big Picture\n",
        "- **Capacity vs. generalization:** More neurons and layers increase capacity but can lead to overfitting.  \n",
        "- **Learning dynamics:** Dropout, learning rate, and batch size tune *how* the model learns, not just how large it is.  \n",
        "- **Defaults matter:** ReLU, a moderate learning rate (0.001), and a medium batch size (64) are reliable starting points.  \n",
        "- **On MNIST:** Even modest networks perform very well — the key lesson is learning to balance trade-offs.  \n",
        "\n",
        "**Key insight:**  \n",
        "Tuning hyperparameters is about **finding balance**: capacity vs. simplicity, speed vs. stability, learning vs. overfitting.  "
      ],
      "metadata": {
        "id": "maFQIEAs69l8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQmNOEHDG_5h"
      },
      "source": [
        "##**Experiment 0: Running the model with 0 neurons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_qoRrpRC4Cc"
      },
      "outputs": [],
      "source": [
        "# Function to build a simple model with no hidden neurons\n",
        "def build_simple_model(init_bias=0.0):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28, 28)))  # Flatten the input images\n",
        "    model.add(Dense(10, activation='softmax', bias_initializer=Constant(init_bias)))  # Output layer\n",
        "    return model\n",
        "\n",
        "# Code to run the experiment with 0 hidden neurons\n",
        "print(\"Running model with 0 neurons.\")\n",
        "simple_model = build_simple_model()\n",
        "history = train_model(simple_model, learning_rate=0.001, batch_size=64, epochs=5)\n",
        "plot_learning_curves(history)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How did the performance of the model with 0 hidden neurons compare to previous models?\n",
        "#    - Analyze the training and validation accuracy and loss values. What does this indicate about the model's capacity to learn?\n",
        "# 2. What limitations does a model with no hidden neurons have in terms of learning complex patterns?\n",
        "#    - Discuss the implications of using a simple model for tasks that may require deeper architectures.\n",
        "# 3. How does the initial bias value affect the model's predictions?\n",
        "#    - Consider changing the `init_bias` parameter and observing any differences in performance. What role does bias play in neural networks?\n",
        "# 4. Did you observe any signs of overfitting or underfitting in the learning curves?\n",
        "#    - Reflect on whether the model struggles to generalize based on the training versus validation performance.\n",
        "# 5. In what scenarios might a simple model be useful despite its limitations?\n",
        "#    - Discuss potential applications or situations where a basic model could provide insights or baseline performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting the Training Log (5 Epochs)\n",
        "\n",
        "This is the step-by-step record of how the model trained across 5 epochs.  \n",
        "Each line shows the model’s performance on both the training set and the validation set.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Epoch 1\n",
        "- **Train Accuracy:** 76.7%  \n",
        "- **Train Loss:** 0.9162  \n",
        "- **Validation Accuracy:** 90.9%  \n",
        "- **Validation Loss:** 0.3408  \n",
        "- Interpretation: the model starts with fairly low training accuracy, but surprisingly high validation accuracy.  \n",
        "  - This happens because dropout and regularization are applied during training but not during validation, so validation looks better.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Epoch 2\n",
        "- **Train Accuracy:** 90.5%  \n",
        "- **Train Loss:** 0.3421  \n",
        "- **Validation Accuracy:** 91.8%  \n",
        "- **Validation Loss:** 0.2974  \n",
        "- Interpretation: huge jump in training accuracy, validation also improving.  \n",
        "  - The model is learning the digit patterns quickly.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Epoch 3\n",
        "- **Train Accuracy:** 91.2%  \n",
        "- **Train Loss:** 0.3167  \n",
        "- **Validation Accuracy:** 92.3%  \n",
        "- **Validation Loss:** 0.2795  \n",
        "- Interpretation: steady progress — both training and validation are improving together.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Epoch 4\n",
        "- **Train Accuracy:** 91.7%  \n",
        "- **Train Loss:** 0.2988  \n",
        "- **Validation Accuracy:** 92.8%  \n",
        "- **Validation Loss:** 0.2698  \n",
        "- Interpretation: small but consistent improvements; no sign of overfitting.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Epoch 5\n",
        "- **Train Accuracy:** 92.4%  \n",
        "- **Train Loss:** 0.2754  \n",
        "- **Validation Accuracy:** 92.8%  \n",
        "- **Validation Loss:** 0.2654  \n",
        "- Interpretation: model has nearly plateaued; training and validation results are very close, showing strong generalization.  \n",
        "\n",
        "---\n",
        "\n",
        "### Overall Takeaways\n",
        "- The model improved rapidly in the first 2 epochs, then slowed down as it approached a stable solution.  \n",
        "- Validation accuracy stayed slightly ahead of training accuracy — normal behavior when dropout/regularization are applied.  \n",
        "- Both training and validation loss steadily decreased, showing effective learning.  \n",
        "- No signs of overfitting (training accuracy is not much higher than validation).  \n",
        "\n",
        "**Key takeaway:**  \n",
        "By epoch 5, the model has reached a solid accuracy (~92–93%) and is well-generalized. More epochs might improve results slightly, but the curve suggests it’s close to its natural performance limit for this architecture.  "
      ],
      "metadata": {
        "id": "ceMrfpJg6Cvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting the Learning Curves\n",
        "\n",
        "These two plots show how our model’s performance changes as it trains over 5 epochs.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Accuracy (left plot)\n",
        "- **Train Accuracy (blue):** starts around 85% and climbs steadily above 92%.  \n",
        "- **Validation Accuracy (orange):** starts slightly higher than training and also climbs, ending around 93%.  \n",
        "- Both curves improve together, which suggests the model is **learning useful patterns** rather than memorizing.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Loss (right plot)\n",
        "- **Train Loss (blue):** starts high (~0.59) and drops quickly to ~0.28.  \n",
        "- **Validation Loss (orange):** starts lower and also decreases smoothly, ending around ~0.26.  \n",
        "- The fact that both training and validation loss are going down means the model is generalizing well.  \n",
        "\n",
        "---\n",
        "\n",
        "#### What This Tells Us\n",
        "- The model is learning effectively — accuracy is rising, loss is falling.  \n",
        "- Validation performance is slightly ahead of training performance. This often happens because:  \n",
        "  - Dropout or other regularization is off during validation, making evaluation a little “easier.”  \n",
        "  - The validation set may simply be slightly less noisy than the training set.  \n",
        "- Importantly, there are **no signs of overfitting yet** (where validation accuracy would stall or drop while training accuracy keeps rising).  \n",
        "\n",
        "---\n",
        "\n",
        "**Key takeaway:**  \n",
        "The network is training successfully: it’s improving steadily, it generalizes to unseen data, and it has not yet overfit. With more epochs, accuracy might improve a bit more, but we’d need to watch carefully for divergence between training and validation curves.  "
      ],
      "metadata": {
        "id": "38CNqnsB6FLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf2JkRymHPa5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Example usage: Display probabilities for a random test image\n",
        "index = np.random.randint(0, len(x_test))\n",
        "display_prediction_probabilities(simple_model, x_test[index], y_test[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting the Prediction Probabilities\n",
        "\n",
        "- **True Label:** X  \n",
        "- **Predicted Label:** Y  \n",
        "\n",
        "---\n",
        "\n",
        "#### What the Chart Shows\n",
        "- The bar for digit **Y** (the model’s prediction) is the tallest, showing the digit the model believes is most likely.  \n",
        "- The height of that bar represents the model’s **confidence** (probability between 0 and 1).  \n",
        "- Other bars show how much probability the model gave to alternative digits.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Possible Scenarios\n",
        "1. **Confident and Correct**  \n",
        "   - If the tallest bar is on the true label (X = Y) and close to 1.0 → the model is sure and correct.  \n",
        "\n",
        "2. **Confident but Wrong**  \n",
        "   - If the tallest bar is on the wrong digit (X ≠ Y) and close to 1.0 → the model is overconfident in a mistake.  \n",
        "\n",
        "3. **Uncertain / Confused**  \n",
        "   - If two or more bars are fairly high → the model is unsure.  \n",
        "   - This usually happens when digits look similar (e.g., “3” vs “5”).  \n",
        "\n",
        "---\n",
        "\n",
        "#### Why This Matters\n",
        "- Looking at probabilities tells us **more than just right or wrong**.  \n",
        "- We can see if the model is:  \n",
        "  - **Accurate and confident** (ideal).  \n",
        "  - **Accurate but uncertain** (still useful, but cautious).  \n",
        "  - **Confidently wrong** (dangerous in real applications).  \n",
        "\n",
        "**Key takeaway:**  \n",
        "This visualization helps us peek inside the model’s reasoning process.  \n",
        "We don’t just see the predicted digit — we see the full **confidence distribution** across all digits.  "
      ],
      "metadata": {
        "id": "AAVbMFx25qPz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjC-gao2U6VV"
      },
      "source": [
        "##Experiment 1: Running the model with 64 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc9B-X6Qm_QZ"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# This experiment evaluates how the number of neurons affects the model's capacity to learn.\n",
        "# A model with too few neurons may not learn effectively.\n",
        "\n",
        "# Code to run the experiment with 64 neurons\n",
        "print(\"Running model with 64 neurons in the hidden layer.\")\n",
        "model_64 = build_model(num_neurons=64)\n",
        "history_64_neurons = train_model(model_64)\n",
        "plot_learning_curves(history_64_neurons)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How did the model perform with 64 neurons compared to the baseline?\n",
        "#    - Look at the final training and validation accuracy.\n",
        "# 2. Did the model learn quickly?\n",
        "#    - Examine the learning curves: How many epochs did it take to see significant improvement in accuracy?\n",
        "# 3. How accurate was the model?\n",
        "#    - Focus on the final accuracy values reported in the output after training. What were they for both training and validation sets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frudU8kvakqs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Example usage: Display probabilities for a random test image\n",
        "index = np.random.randint(0, len(x_test))\n",
        "display_prediction_probabilities(model_64, x_test[index], y_test[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnheFvIEluoB"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEAuLYV6VBcH"
      },
      "source": [
        "##Experiment 2: Running the model with 128 neurons (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlI1CLE1nKcn"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# This experiment establishes a baseline for comparing other configurations.\n",
        "\n",
        "# Code to run the experiment with 128 neurons\n",
        "print(\"Running model with 128 neurons (baseline).\")\n",
        "model_128 = build_model(num_neurons=128)\n",
        "history_128_neurons = train_model(model_128)\n",
        "plot_learning_curves(history_128_neurons)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How well did the model perform with 128 neurons?\n",
        "#    - Review the final training and validation accuracy values.\n",
        "# 2. What patterns do you see in the learning curves?\n",
        "#    - Discuss whether the curves indicate overfitting or underfitting.\n",
        "# 3. Did the model generalize well to the validation data?\n",
        "#    - Look for any discrepancies between training and validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sH-jAjMaygn"
      },
      "outputs": [],
      "source": [
        "# Example usage: Display probabilities for a random test image\n",
        "index = np.random.randint(0, len(x_test))\n",
        "display_prediction_probabilities(model_128, x_test[index], y_test[index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg6CHWkMl9Jk"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SAb6HYEVHUy"
      },
      "source": [
        "##Experiment 3: Running the model with 256 neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip-q6xe3nexc"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Increasing the number of neurons may improve performance but could lead to overfitting.\n",
        "\n",
        "# Code to run the experiment with 256 neurons\n",
        "print(\"Running model with 256 neurons in the hidden layer.\")\n",
        "model_256 = build_model(num_neurons=256)\n",
        "history_256_neurons = train_model(model_256)\n",
        "plot_learning_curves(history_256_neurons)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. Did adding more neurons improve the performance of the model?\n",
        "#    - Compare final accuracy with the previous models (64 and 128 neurons).\n",
        "# 2. Did you observe any signs of overfitting?\n",
        "#    - Check for a significant gap between training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSwXv5ika1lg"
      },
      "outputs": [],
      "source": [
        "# Example usage: Display probabilities for a random test image\n",
        "index = np.random.randint(0, len(x_test))\n",
        "display_prediction_probabilities(model_256, x_test[index], y_test[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1040vbgil_fB"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d6QFbeuVMs_"
      },
      "source": [
        "##Experiment 4: Running the model with 1 hidden layer (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8svlrhUoQ8B"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# This experiment examines the importance of network depth.\n",
        "\n",
        "# Code to run the experiment with 1 hidden layer\n",
        "print(\"Running model with 1 hidden layer (baseline).\")\n",
        "model_1_layer = build_model(num_hidden_layers=1)\n",
        "history_1_hidden_layer = train_model(model_1_layer)\n",
        "plot_learning_curves(history_1_hidden_layer)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model perform with just one hidden layer?\n",
        "#    - Look at the training and validation accuracy.\n",
        "# 2. Are there any signs of underfitting or overfitting?\n",
        "#    - Compare performance against deeper models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v34V6qf0mAtU"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZWQSQEBVPO8"
      },
      "source": [
        "##Experiment 5: Running the model with 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-fQvKIvofHz"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# More hidden layers allow for better feature extraction and can improve performance.\n",
        "\n",
        "# Code to run the experiment with 2 hidden layers\n",
        "print(\"Running model with 2 hidden layers.\")\n",
        "model_2_layers = build_model(num_hidden_layers=2)\n",
        "history_2_hidden_layers = train_model(model_2_layers)\n",
        "plot_learning_curves(history_2_hidden_layers)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. Did the addition of a second hidden layer improve the model's performance?\n",
        "#    - Compare the final accuracy with the one-layer model.\n",
        "# 2. What do you observe in the learning curves?\n",
        "#    - Analyze convergence speed and stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVctWwVPmDMl"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_k5crAeVR2p"
      },
      "source": [
        "##Experiment 6: Running the model with 3 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unoqYOnko0X_"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Increasing complexity may lead to improved performance but also raises the risk of overfitting.\n",
        "\n",
        "# Code to run the experiment with 3 hidden layers\n",
        "print(\"Running model with 3 hidden layers.\")\n",
        "model_3_layers = build_model(num_hidden_layers=3)\n",
        "history_3_hidden_layers = train_model(model_3_layers)\n",
        "plot_learning_curves(history_3_hidden_layers)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model performance change with three hidden layers?\n",
        "#    - Compare the final accuracy with models having fewer layers.\n",
        "# 2. Are there any noticeable differences in convergence speed or final accuracy?\n",
        "#    - Look for any improvement in learning stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zZmuCromDnW"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbpXwgnhVUUj"
      },
      "source": [
        "##Experiment 7: Running with a smaller learning rate (0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbD0UucaVsGp"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# A smaller learning rate can lead to more stable training but may slow convergence.\n",
        "\n",
        "# Code to run the experiment with a smaller learning rate\n",
        "print(\"Running model with a smaller learning rate (0.0001).\")\n",
        "model_small_lr = build_model()\n",
        "history_small_lr = train_model(model_small_lr, learning_rate=0.0001)\n",
        "plot_learning_curves(history_small_lr)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How did the smaller learning rate affect the training speed and accuracy?\n",
        "#    - Look at the number of epochs taken to achieve a certain accuracy.\n",
        "# 2. Did the model reach a lower final accuracy compared to the default learning rate?\n",
        "#    - Compare results with the baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoBRWZlSmD8N"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAvhnnElVz4g"
      },
      "source": [
        "##Experiment 8: Running with a Larger Learning Rate (0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAo8uizTV4Do"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# A larger learning rate can lead to faster convergence but may cause instability.\n",
        "\n",
        "# Code to run the experiment with a larger learning rate\n",
        "print(\"Running model with a larger learning rate (0.01).\")\n",
        "model_large_lr = build_model()\n",
        "history_large_lr = train_model(model_large_lr, learning_rate=0.01)\n",
        "plot_learning_curves(history_large_lr)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. Did the larger learning rate help the model converge faster?\n",
        "#    - Assess the learning curves for convergence speed.\n",
        "# 2. Were there any signs of instability in the training process?\n",
        "#    - Look for erratic behavior in the loss or accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChm7U4cmENR"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9798-WLpV7Nt"
      },
      "source": [
        "##Experiment 9: Running with Smaller Batch Size (32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R664iuhgXa4s"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Smaller batch sizes can introduce more variability in training.\n",
        "\n",
        "# Code to run the experiment with a smaller batch size\n",
        "print(\"Running model with smaller batch size (32).\")\n",
        "model_small_batch = build_model()\n",
        "history_small_batch = train_model(model_small_batch, batch_size=32)\n",
        "plot_learning_curves(history_small_batch)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model perform with a smaller batch size?\n",
        "#    - Compare the final accuracy with the larger batch size.\n",
        "# 2. Is there more fluctuation in the learning curves compared to the baseline?\n",
        "#    - Analyze the stability of accuracy and loss curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnqtE3sGmEjb"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw2LoZ-XXeBg"
      },
      "source": [
        "##Experiment 10: Running with Larger Batch Size (128)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3o9WB0xXijO"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Larger batch sizes lead to smoother training but may slow down convergence.\n",
        "\n",
        "# Code to run the experiment with a larger batch size\n",
        "print(\"Running model with larger batch size (128).\")\n",
        "model_large_batch = build_model()\n",
        "history_large_batch = train_model(model_large_batch, batch_size=128)\n",
        "plot_learning_curves(history_large_batch)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model perform with a larger batch size?\n",
        "#    - Look at the accuracy and loss values compared to smaller sizes.\n",
        "# 2. Does it show smoother learning curves, and what impact does that have on training time?\n",
        "#    - Assess convergence and overall stability of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od0-P4AgmEyG"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trBLaL1OXp38"
      },
      "source": [
        "##Experiment 11: Running the Model with Dropout (0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCUN63XVXtOc"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Dropout is a regularization technique to prevent overfitting.\n",
        "\n",
        "# Code to run the experiment with dropout\n",
        "print(\"Running model with dropout rate of 0.2.\")\n",
        "model_dropout_02 = build_model(dropout_rate=0.2)\n",
        "history_dropout_02 = train_model(model_dropout_02)\n",
        "plot_learning_curves(history_dropout_02)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model perform with a dropout rate of 0.2?\n",
        "#    - Compare training accuracy with the baseline model.\n",
        "# 2. Is there a noticeable difference in accuracy compared to a model without dropout?\n",
        "#    - Analyze the impact of dropout on validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D23XV555mFFx"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSMuLdQ1Xvep"
      },
      "source": [
        "##Experiment 12: Running the Model with Dropout (0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvyXeb4IXzaG"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Increasing dropout can further help in reducing overfitting but may lower training accuracy.\n",
        "\n",
        "# Code to run the experiment with dropout\n",
        "print(\"Running model with dropout rate of 0.5.\")\n",
        "model_dropout_05 = build_model(dropout_rate=0.5)\n",
        "history_dropout_05 = train_model(model_dropout_05)\n",
        "plot_learning_curves(history_dropout_05)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model perform with a dropout rate of 0.5?\n",
        "#    - Check final accuracy compared to lower dropout rates.\n",
        "# 2. Does a higher dropout rate significantly affect training accuracy?\n",
        "#    - Analyze the balance between overfitting and performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UikGj840mFWv"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PXywuCIX1ja"
      },
      "source": [
        "##Experiment 13: Using Sigmoid Activation Instead of ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV_VUSXWX6bm"
      },
      "outputs": [],
      "source": [
        "# Concept Explanation:\n",
        "# Different activation functions can impact the learning dynamics of the model.\n",
        "\n",
        "# Code to run the experiment with sigmoid activation\n",
        "print(\"Running model with sigmoid activation.\")\n",
        "model_sigmoid = build_model(activation='sigmoid')\n",
        "history_sigmoid = train_model(model_sigmoid)\n",
        "plot_learning_curves(history_sigmoid)\n",
        "\n",
        "# Questions to consider:\n",
        "# 1. How does the model perform with sigmoid activation compared to ReLU?\n",
        "#    - Assess final accuracy and learning stability.\n",
        "# 2. Did you notice any difference in the speed of learning or final performance?\n",
        "#    - Discuss the implications of using different activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahZHyUv-mFiV"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwrZBs6imjdw"
      },
      "source": [
        "##Overall Observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4IYw0Z5mddh"
      },
      "source": [
        "Based on the results of the 14 experiments, how do different hyperparameters, model architectures, and training techniques impact the performance of neural networks compared to traditional classification methods? In your response, consider aspects such as accuracy, loss, generalization, and computational efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP63BR7Smpf0"
      },
      "source": [
        "**Your Observations:** xxx"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}